{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione lineare multipla\n",
    "La regressione lineare multipla o multivariata √® un caso di regressione lineare con due o pi√π variabili indipendenti.\n",
    "\n",
    "Se ci sono solo due variabili indipendenti, la funzione di regressione stimata √® ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ. Rappresenta un piano di regressione in uno spazio tridimensionale. L'obiettivo della regressione √® determinare i valori dei pesi ùëè‚ÇÄ, ùëè‚ÇÅ e ùëè‚ÇÇ in modo tale che questo piano sia il pi√π vicino possibile alle risposte effettive, producendo al contempo l'SSR minimo.\n",
    "\n",
    "Il caso di pi√π di due variabili indipendenti √® simile, ma pi√π generale. La funzione di regressione stimata √® ùëì(ùë•‚ÇÅ, ‚Ä¶, ùë•·µ£) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ‚ãØ +ùëè·µ£ùë•·µ£, e ci sono ùëü + 1 pesi da determinare quando il numero di input √® ùëü."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementiamo la Regressione lineare multipla con scikit-learn\n",
    "√à possibile implementare una regressione lineare multipla seguendo gli stessi passaggi della regressione semplice. La differenza principale √® che ora il nostro x array avr√† due o pi√π colonne.\n",
    "\n",
    "#### Passaggi 1 e 2: importiamo pacchetti e classi e forniamo i dati\n",
    "\n",
    "Innanzitutto, importiamo numpy e sklearn.linear_model.LinearRegression e forniamo input e output noti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo gi√† detto, nella regressione lineare multipla, x √® una matrice bidimensionale con almeno due colonne, mentre y di solito √® una matrice unidimensionale. Questo √® un semplice esempio di regressione lineare multipla e x ha esattamente due colonne.\n",
    "\n",
    "### Passaggio 3: creiamo un modello e lo adattiamo\n",
    "\n",
    "Il passaggio successivo consiste nel creare il modello di regressione come istanza LinearRegressione per poi allenarlo con .fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il risultato di questa istruzione √® la variabile model che fa riferimento all'oggetto di tipo LinearRegression. Rappresenta il modello di regressione adattato ai dati esistenti.\n",
    "\n",
    "### Passaggio 4: ottieniamo i risultati\n",
    "\n",
    "√à possibile ottenere le propriet√† del modello come nel caso della regressione lineare semplice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.8615939258756776\n",
      "intercept: 5.52257927519819\n",
      "coefficients: [0.44706965 0.25502548]\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "\n",
    "print(f\"intercept: {model.intercept_}\")\n",
    "\n",
    "\n",
    "print(f\"coefficients: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ottiene il valore di ùëÖ¬≤ utilizzando .score() i valori degli stimatori dei coefficienti di regressione con .intercept_e .coef_. Ancora una volta, .intercept_ mantiene il bias ùëè‚ÇÄ, mentre ora .coef_√® un array contenente ùëè‚ÇÅ e ùëè‚ÇÇ.\n",
    "\n",
    "In questo esempio, l'intercetta √® circa 5,52 e questo √® il valore della risposta prevista quando ùë•‚ÇÅ = ùë•‚ÇÇ = 0. Un aumento di ùë•‚ÇÅ di 1 produce un aumento della risposta prevista di 0,45. Allo stesso modo, quando ùë•‚ÇÇ cresce di 1, la risposta aumenta di 0,26.\n",
    "\n",
    "### Passaggio 5: prevedere la risposta\n",
    "\n",
    "Anche le previsioni funzionano allo stesso modo del caso della regressione lineare semplice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La risposta prevista si ottiene con .predict(), che equivale alla seguente formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.intercept_ + np.sum(model.coef_ * x, axis=1)\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√à possibile prevedere i valori di output moltiplicando ciascuna colonna dell'input con il peso appropriato, per poi sommare questo valore con l'intercetta.\n",
    "\n",
    "Possiamo applicare questo modello anche a nuovi dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.77760476,  7.18179502,  8.58598528,  9.99017554, 11.3943658 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primo Esempio pratico\n",
    "### Analizzando il dataset cardata.csv possiamo prevedere le emissioni CO2 in base alle dimensioni del motore e al peso dell'auto, con la possibilit√† di analizzare 2 carretteristiche invece che una sola possiamo infatti rendere la previsione pi√π accurata.\n",
    "\n",
    "\n",
    "Prima cosa da fare importiamo scikit-learn per il modello e pandas per leggere il csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Car       Model  Volume  Weight  CO2\n",
      "0       Toyoty        Aygo    1000     790   99\n",
      "1   Mitsubishi  Space Star    1200    1160   95\n",
      "2        Skoda      Citigo    1000     929   95\n",
      "3         Fiat         500     900     865   90\n",
      "4         Mini      Cooper    1500    1140  105\n",
      "5           VW         Up!    1000     929  105\n",
      "6        Skoda       Fabia    1400    1109   90\n",
      "7     Mercedes     A-Class    1500    1365   92\n",
      "8         Ford      Fiesta    1500    1112   98\n",
      "9         Audi          A1    1600    1150   99\n",
      "10     Hyundai         I20    1100     980   99\n",
      "11      Suzuki       Swift    1300     990  101\n",
      "12        Ford      Fiesta    1000    1112   99\n",
      "13       Honda       Civic    1600    1252   94\n",
      "14      Hundai         I30    1600    1326   97\n",
      "15        Opel       Astra    1600    1330   97\n",
      "16         BMW           1    1600    1365   99\n",
      "17       Mazda           3    2200    1280  104\n",
      "18       Skoda       Rapid    1600    1119  104\n",
      "19        Ford       Focus    2000    1328  105\n",
      "20        Ford      Mondeo    1600    1584   94\n",
      "21        Opel    Insignia    2000    1428   99\n",
      "22    Mercedes     C-Class    2100    1365   99\n",
      "23       Skoda     Octavia    1600    1415   99\n",
      "24       Volvo         S60    2000    1415   99\n",
      "25    Mercedes         CLA    1500    1465  102\n",
      "26        Audi          A4    2000    1490  104\n",
      "27        Audi          A6    2000    1725  114\n",
      "28       Volvo         V70    1600    1523  109\n",
      "29         BMW           5    2000    1705  114\n",
      "30    Mercedes     E-Class    2100    1605  115\n",
      "31       Volvo        XC70    2000    1746  117\n",
      "32        Ford       B-Max    1600    1235  104\n",
      "33         BMW         216    1600    1390  108\n",
      "34        Opel      Zafira    1600    1405  109\n",
      "35    Mercedes         SLK    2500    1395  120\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import linear_model\n",
    "df = pandas.read_csv(\"dati/dataset/cardata.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizzato il dataset possiamo creare le nostre variabili:\n",
    "- X per contenente i valori indipendenti;\n",
    "- y per contenere i valori target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi utilizziamo il nostro modello e il metodo .fit() per allenarerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['Weight', 'Volume']]\n",
    "y = df['CO2']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo prevedere i valori di CO2 in base al peso e al volume di un'auto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107.2087328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#predict the CO2 emission of a car where the weight is 2300kg, and the volume is 1300cm3:\n",
    "predictedCO2 = regr.predict([[2300, 1300]])\n",
    "print(predictedCO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passiamo al Coefficiente\n",
    "Abbiamo detto che il coefficiente √® un fattore che descrive la relazione con una variabile sconosciuta.\n",
    "\n",
    "In questo caso, possiamo chiedere il valore del coefficiente di peso rispetto a CO2 e di volume rispetto a CO2. Le risposte che otteniamo ci dicono cosa accadrebbe se aumentassimo o diminuissimo uno dei valori indipendenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00755095 0.00780526]\n"
     ]
    }
   ],
   "source": [
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice dei risultati rappresenta i valori dei coefficienti di peso e volume.\n",
    "\n",
    "Peso: 0,00755095\n",
    "Volume: 0,00780526\n",
    "\n",
    "Questi valori ci dicono che se il peso aumenta di 1kg, l'emissione di CO2 aumenta di 0,00755095g.\n",
    "\n",
    "E se la cilindrata (Volume) aumenta di 1 centimentro cubo, l'emissione di CO2 aumenta di 0,00780526 g.\n",
    "\n",
    "Considerando che sia una supposizione giusta andiamo a fare altre prove.\n",
    "\n",
    "Abbiamo gi√† previsto che se un'auto con motore da 1300 cm 3 pesa 2300 kg, l'emissione di CO2 sar√† di circa 107 g.\n",
    "\n",
    "Cosa succede se aumentiamo il peso di 1000 kg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.75968007]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictedCO2 = regr.predict([[3300, 1300]])\n",
    "\n",
    "print(predictedCO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo previsto che un'auto con motore da 1,3 litri e peso di 3300 kg rilascer√† circa 115 grammi di CO2 per ogni chilometro percorso.\n",
    "\n",
    "Ci√≤ dimostra che il coefficiente di 0,00755095 √® corretto:\n",
    "\n",
    "107,2087328 + (1000 * 0,00755095) = 114,75968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primo Esercizio\n",
    " Partendo dal dataset a questo link https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression , utilizzate i dati sulle ore di studio e le ore di sonno per prevedere quanto queste caratteristiche influiscono sull'indice di prestazione degli studenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secono Esercizio\n",
    "Partendo dal dataset a questo link https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction , utilizzate i dati sugli anni delle case e la distanza dalla stazione metro per prevedere quanto queste caratteristiche influiscono sul costo delle case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL = \"data/weather/trained_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINED_MODEL, \"wb\") as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL = \"data/weather/trained_model.pkl\"\n",
    "\n",
    "with open(TRAINED_MODEL, \"rb\") as f:\n",
    "    trained_regressor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione polinomiale\n",
    "Se i nostri punti chiaramente non si adattano a una regressione lineare (una linea retta che passa attraverso tutti i punti dati), potrebbe essere ideale utilizzare la regressione polinomiale.\n",
    "\n",
    "La regressione polinomiale, come la regressione lineare, utilizza la relazione tra le variabili X e y per trovare il modo migliore per tracciare una linea attraverso i punti dati.\n",
    "\n",
    "\n",
    "\n",
    "![grafico](./dati/img/img_polynomial_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressione polinomiale con scikit-learn\n",
    "L'implementazione della regressione polinomiale con scikit-learn √® molto simile alla regressione lineare. Ma c'√® solo un passaggio in pi√π: dobbiamo trasformare l'array di input per includere termini non lineari come ùë•¬≤.\n",
    "\n",
    "### Passaggio 1: importiamo pacchetti e classi\n",
    "\n",
    "Oltre a numpy e sklearn.linear_model.LinearRegression, dobbiamo anche importare la classe PolynomialFeatures da sklearn.preprocessing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'importazione √® ora completata e abbiamo tutto ci√≤ di cui abbiamo bisogno per lavorare.\n",
    "\n",
    "### Passaggio 2a: forniamo i dati\n",
    "\n",
    "Questo passaggio definisce l'input e l'output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([15, 11, 2, 8, 25, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora abbiamo l'input e l'output in un formato adatto. Dobbiamo ricordarci che √® necessario che l'input sia un array bidimensionale . Ecco perch√© .reshape() viene utilizzato.\n",
    "\n",
    "### Passaggio 2b: trasformarmiamo i dati di input\n",
    "\n",
    "Questo √® il nuovo passaggio che dobbiamo implementare per la regressione polinomiale!\n",
    "\n",
    "Come abbiamo imparato in precedenza, dobbiamo includere ùë•¬≤ come funzionalit√† aggiuntive quando implementiamo la regressione polinomiale. Per questo motivo, dobbiamo trasformare l'array di input X per contenere eventuali colonne aggiuntive con i valori di ùë•¬≤ ed eventualmente pi√π funzionalit√†.\n",
    "\n",
    "√à possibile trasformare l'array di input in diversi modi, ad esempio utilizzando insert() from numpy. Ma nel nostro caso la funzione PolynomialFeatures √® molto pi√π conveniente per questo scopo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variabile transformer si riferisce a un'istanza PolynomialFeatures che √® possibile utilizzare per trasformare l'input X.\n",
    "\n",
    "√à possibile fornire diversi parametri facoltativi per PolynomialFeatures:\n",
    "\n",
    "- degree √® un numero intero ( 2 per impostazione predefinita) che rappresenta il grado della funzione di regressione polinomiale.\n",
    "- interaction_only √® un valore booleano ( False per impostazione predefinita) che decide se includere solo le funzionalit√† di interazione ( True) o tutte le funzionalit√† ( False).\n",
    "- include_bias √® un valore booleano ( True per impostazione predefinita) che decide se includere la colonna di valori ( True) o meno ( False).\n",
    "\n",
    "In questo esempio vengono utilizzati i valori predefiniti di tutti i parametri tranne include_bias. A volte vorrai sperimentare il grado della funzione e pu√≤ essere utile per la leggibilit√† fornire comunque questo argomento.\n",
    "\n",
    "Prima di applicare transformer √® necessario munirlo di .fit(), una volta fatto √® pronto per creare un nuovo array di input modificato. Utilizziamo .transform() per farlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(x)\n",
    "x_ = transformer.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa trasformazione dell'array di input con .transform() prende l'array di input come argomento e restituisce l'array modificato.\n",
    "\n",
    "Per velocit√† si pu√≤ anche usare .fit_transform() per sostituire le tre affermazioni precedenti con una sola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con .fit_transform(), stiamo adattando e trasformando l'array di input in un'unica istruzione. Anche questo metodo accetta l'array di input e fa effettivamente la stessa cosa di .fit() e .transform()c hiamato in quest'ordine. Restituisce anche l'array modificato. Ecco come appare il nuovo array di input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'array di input modificato contiene due colonne: una con gli input originali e l'altra con i relativi quadrati.\n",
    "\n",
    "### Passaggio 3: creiamo un modello e adattalo\n",
    "\n",
    "Anche questo passaggio √® lo stesso del caso della regressione lineare. Creiamo e adattiamo il ‚Äã‚Äãmodello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello di regressione √® ora creato e adattato. √à pronto per l'applicazione. Ricordiamoci che il primo argomento di .fit() √® l' array di input modificato x_ e non l'originale x.\n",
    "\n",
    "### Passaggio 4: ottieniamo i risultati\n",
    "\n",
    "√à possibile ottenere le propriet√† del modello allo stesso modo del caso della regressione lineare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq = model.score(x_, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "\n",
    "print(f\"intercept: {model.intercept_}\")\n",
    "\n",
    "\n",
    "print(f\"coefficients: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancora una volta, .score() ritorna ùëÖ¬≤. Anche il suo primo argomento √® l'input modificato x_, non x. I valori dei pesi sono associati a .intercept_ e .coef_. Qui .intercept_ rappresenta ùëè‚ÇÄ, mentre .coef_ fa riferimento all'array che contiene ùëè‚ÇÅ e ùëè‚ÇÇ.\n",
    "\n",
    "### Passaggio 5: prevedere la risposta\n",
    "\n",
    "Se vogliamo ottenere la risposta prevista, usiamo semplicemente .predict(), ma ricordiamo che l'argomento dovrebbe essere l'input modificato x_ anzich√© il vecchio x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_)\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo vedere, la previsione funziona quasi allo stesso modo del caso della regressione lineare. Richiede solo l'input modificato anzich√© l'originale.\n",
    "\n",
    "√à possibile applicare una procedura identica se si hanno pi√π variabili di input . Avremo un array di input con pi√π di una colonna, ma tutto il resto sar√† lo stesso. Ecco un esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import packages and classes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 2 a: Provide data\n",
    "x = [\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "# Step 2 b: Transform input data\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Step 3: Create a model and fit it\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# Step 4: Get results\n",
    "r_sq = model.score(x_, y)\n",
    "intercept, coefficients = model.intercept_, model.coef_\n",
    "\n",
    "# Step 5: Predict response\n",
    "y_pred = model.predict(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo esempio di regressione produce i seguenti risultati e previsioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "\n",
    "print(f\"intercept: {intercept}\")\n",
    "\n",
    "\n",
    "print(f\"coefficients:\\n{coefficients}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo notare che la regressione polinomiale ha prodotto un coefficiente di determinazione pi√π elevato rispetto alla regressione lineare multipla per lo stesso problema. Inizialmente si potrebbe pensare che ottenere un ùëÖ¬≤ cos√¨ grande sia un ottimo risultato.\n",
    "\n",
    "Tuttavia, nelle situazioni del mondo reale, anche avere un modello complesso e ùëÖ¬≤ molto vicino a uno potrebbe essere un problema di overfitting (di cui parleremo a brevissimo). Per verificare le prestazioni di un modello, dovremmo testarlo con nuovi dati, ovvero con osservazioni non utilizzate per adattare o addestrare il modello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altro esempio\n",
    "\n",
    "Nell'esempio seguente abbiamo immortalato 18 automobili mentre transitavano davanti ad un determinato casello.\n",
    "\n",
    "Abbiamo registrato la velocit√† dell'auto e l'ora del giorno in cui √® avvenuto il passaggio.\n",
    "\n",
    "L'asse x rappresenta le ore del giorno e l'asse y rappresenta la velocit√†, iniziamo disegnando un grafico a dispersione:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\n",
    "y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usiamo numpy per creare un modello polinomiale:\n",
    "import numpy\n",
    "mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))\n",
    "\n",
    "#Quindi specifichiamo come verr√† visualizzata la linea, iniziamo dalla posizione 1 e terminiamo alla posizione 22:\n",
    "\n",
    "myline = numpy.linspace(1, 22, 100)\n",
    "\n",
    "#Disegnamo il grafico a dispersione originale:\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "#Disegna la linea di regressione polinomiale:\n",
    "\n",
    "plt.plot(myline, mymodel(myline))\n",
    "\n",
    "#Visualizza il diagramma:\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturalmente √® importante sapere quanto √® buona la relazione tra i valori degli assi x e y, se non esiste alcuna relazione la regressione polinomiale non pu√≤ essere utilizzata per prevedere nulla.\n",
    "\n",
    "Misuriamo la relazione con il valore r-quadrato che ricordiamo varia da 0 a 1, dove 0 significa nessuna relazione e 1 significa correlato al 100%.\n",
    "\n",
    "Come abbiamo gi√† visto, Python e il modulo Sklearn calcoleranno questo valore per noi, tutto ci√≤ che dobbiamo fare √® alimentarlo con gli array X e y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\n",
    "y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n",
    "\n",
    "mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))\n",
    "\n",
    "print(r2_score(y, mymodel(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il risultato 0,94 mostra che esiste un'ottima relazione e possiamo utilizzare la regressione polinomiale nelle previsioni future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ora torniamo alle slide per parlare dell'overfitting e underfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
