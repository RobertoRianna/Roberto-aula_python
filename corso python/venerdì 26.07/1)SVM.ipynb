{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approfondiamo l'algoritmo Support Vector Machines\n",
    "Abbiamo detto che le Support Vector Machines (SVM) sono una classe particolarmente potente e flessibile di algoritmi supervisionati sia per la classificazione, in questi esempi svilupperemo l'intuizione dietro le macchine a vettori di supporto e il loro utilizzo nei problemi di classificazione.\n",
    "\n",
    "Iniziamo con le importazioni standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "#usiamo le impostazioni predefinite di plottaggio seaborn \n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macchine vettoriali di supporto \n",
    "Come abbiamo già detto, le SVM fanno parte della classificazione discriminativa : anziché modellare ciascuna classe, troviamo semplicemente una linea o curva (in due dimensioni) o una varietà (in più dimensioni) che divide le classi l'una dall'altra.\n",
    "\n",
    "Ad esempio, consideriamo il semplice caso di un compito di classificazione, in cui le due classi di punti sono ben separate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo detto, un classificatore discriminativo lineare tenterà di tracciare una linea retta che separa i due insiemi di dati e quindi di creare un modello per la classificazione. Per dati bidimensionali come quelli mostrati qui, questo è un compito che potremmo svolgere manualmente. Ma come detto in precedenza subito si presenterebbe un problema: esiste più di una possibile linea di demarcazione che possa discriminare perfettamente le due classi!\n",
    "\n",
    "Andiamo a vederle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tratta di tre separatori molto diversi che, tuttavia, discriminano perfettamente questi campioni. A seconda di quello che scegliamo, a un nuovo punto dati (ad esempio, quello contrassegnato dalla \"X\" in questo grafico) verrà assegnata un'etichetta diversa! Quindi evidentemente la nostra semplice intuizione di “tracciare una linea tra le classi” non è sufficiente, e dobbiamo pensare un po’ più a fondo.\n",
    "\n",
    "## Macchine vettoriali di supporto: massimizzare margine\n",
    "Le macchine vettoriali di supporto offrono appunto un modo per migliorare questo aspetto. Come detto l'intuizione è questa: invece di tracciare semplicemente una linea di larghezza zero tra le classi, possiamo tracciare attorno a ciascuna linea un margine di una certa larghezza, fino al punto più vicino. Ecco un esempio di come potrebbe apparire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come già specificato nelle macchine a vettori di supporto, la linea che massimizza questo margine è quella che sceglieremo come modello ottimale. \n",
    "\n",
    "### Montaggio di una macchina vettoriale di supporto\n",
    "Vediamo il risultato di un adattamento effettivo a questi dati: utilizzeremo il classificatore vettoriale di supporto di Scikit-Learn per addestrare un modello SVM su questi dati. Per il momento utilizzeremo un kernel lineare e imposteremo il parametro C su un numero molto grande:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per visualizzare meglio cosa sta succedendo qui, creiamo una funzione rapida che traccerà per noi i limiti decisionali della SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Creiamo la funzione decisionale per un SVC 2D\"\"\" \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "#creiamo il grafico\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn',edgecolors='black')\n",
    "#richiamimamo la funzione\n",
    "plot_svc_decision_function(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa è la linea di demarcazione che massimizza il margine tra i due insiemi di punti. Notiamo che alcuni punti di allenamento toccano appena il margine, questi punti sono gli elementi cardine di questo adattamento e sono noti come vettori di supporto e danno il nome all'algoritmo. In Scikit-Learn, l'identità di questi punti è memorizzata nell'attributo support_vectors_ del classificatore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.support_vectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una chiave del successo di questo classificatore è che per l'adattamento conta solo la posizione dei vettori di supporto; eventuali punti più lontani dal margine che si trovano sul lato corretto non modificano l'adattamento! Tecnicamente, questo accade perché questi punti non contribuiscono alla funzione di perdita utilizzata per adattare il modello, quindi la loro posizione e il loro numero non contano finché non oltrepassano il margine.\n",
    "\n",
    "Possiamo vederlo, a esempio, se tracciamo il modello appreso dai primi 60 punti e dai primi 120 punti di questo set di dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn', edgecolors='black')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel pannello di sinistra vediamo il modello e i vettori di supporto per 60 punti di allenamento. Nel pannello di destra abbiamo raddoppiato il numero di punti di allenamento, ma il modello non è cambiato: i tre vettori di supporto del pannello di sinistra sono ancora i vettori di supporto del pannello di destra. Questa insensibilità al comportamento esatto dei punti distanti è uno dei punti di forza del modello SVM.\n",
    "\n",
    "## Oltre i confini lineari: Kernel \n",
    "Il punto in cui SVM diventa estremamente potente è quando viene combinato con i kernel. \n",
    "Per motivare la necessità dei kernel, esaminiamo alcuni dati che non sono separabili linearmente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn', edgecolors='black')\n",
    "plot_svc_decision_function(clf, plot_support=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È chiaro che nessuna discriminazione lineare potrà mai separare questi dati, ma come già spiegato potremmo proiettare i dati in una dimensione più elevata in modo tale che un separatore lineare sia sufficiente. Ad esempio, una semplice proiezione che potremmo utilizzare sarebbe quella di calcolare una funzione di base radiale centrata sul gruppo centrale, per poi visualizzare questa dimensione di dati aggiuntiva utilizzando un grafico tridimensionale: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione\n",
    "r = np.exp(-(X ** 2).sum(1))\n",
    "\n",
    "#grafico 3d\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=10, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn',edgecolors='black')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "plot_3D() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere che con questa dimensione aggiuntiva, i dati diventano banalmente separabili linearmente, disegnando un piano di separazione, diciamo, a r = 0,7.\n",
    "\n",
    "Qui abbiamo dovuto scegliere e mettere a punto con attenzione la nostra proiezione: se non avessimo centrato la nostra funzione di base radiale nella posizione giusta, non avremmo visto risultati così netti e linearmente separabili. In generale, la necessità di fare una scelta del genere è un problema: vorremmo in qualche modo trovare automaticamente le migliori funzioni di base da utilizzare.\n",
    "\n",
    "Una strategia a tal fine è calcolare una funzione di base centrata in ogni punto del set di dati e lasciare che l'algoritmo SVM esamini i risultati. Questo tipo di trasformazione della funzione di base è nota come trasformazione del kernel , poiché si basa su una relazione di somiglianza (o kernel) tra ciascuna coppia di punti.\n",
    "\n",
    "Può sorgere un potenziale problema con questa strategia: la proiezione N punti in N dimensioni potrebbe diventare molto intensivo dal punto di vista computazionale man mano che N diventa grande. Tuttavia, grazie a una piccola procedura nota come trucco del kernel, può essere eseguito implicitamente un adattamento sui dati trasformati dal kernel , ovvero senza mai creare l'intero file.\n",
    "Questo trucco del kernel è integrato nell'SVM ed è uno dei motivi per cui il metodo è così potente.\n",
    "\n",
    "In Scikit-Learn, possiamo applicare SVM kernelizzato semplicemente cambiando il nostro kernel lineare in un kernel RBF (funzione a base radiale), utilizzando il paramentro kernel del modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn',edgecolors='black')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando questa macchina vettoriale di supporto kernelizzato, impariamo un opportuno confine decisionale non lineare. Questa strategia di trasformazione del kernel viene utilizzata spesso nell'apprendimento automatico per trasformare metodi lineari veloci in metodi non lineari veloci, in particolare per i modelli in cui è possibile utilizzare il trucco del kernel.\n",
    "\n",
    "### Ottimizzazione dell'SVM: Attenuazione\n",
    "La nostra discussione finora si è incentrata su set di dati molto puliti, in cui esiste un confine decisionale perfetto. Ma cosa succede se i nostri dati presentano una certa sovrapposizione? A esempio, potremmo avere dati come questi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn',edgecolors='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per gestire questo caso, l'implementazione SVM come abbiamo detto ha un fattore di confusione che \"ammorbidisce\" il margine: ovvero consente ad alcuni punti di insinuarsi nel margine se ciò consente un adattamento migliore. La durezza del margine è controllata da un parametro di regolazione, spesso noto come C. Per C molto grande, il margine è duro e i punti non possono trovarsi al suo interno. Per C più piccoli, il margine è più morbido e può crescere fino a comprendere alcuni punti.\n",
    "\n",
    "La trama mostrata di seguito fornisce un'immagine visiva di come avviene un cambiamento C , il parametro influisce sull'adattamento finale, tramite l'ammorbidimento del margine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn',edgecolors='black')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none')\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andiamo ora a vedere come può essere applicato l'algoritmo SVM con lo stesso problema già visto del riconoscimento di cifre scritte a mano\n",
    "Prima cosa importiamo le librerie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, metrics, svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ricordo che il set di dati delle cifre è costituito da immagini di cifre da 8x8 pixel. L' images attributo del set di dati memorizza matrici 8x8 di valori in scala di grigi per ciascuna immagine. Utilizzeremo questi array per visualizzare le prime 4 immagini. L'attributo target del set di dati memorizza la cifra rappresentata da ciascuna immagine e questa è inclusa nel titolo dei 4 grafici seguenti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per applicare il classificatore a questi dati, procediamo ad appiattire le immagini, trasformando ogni matrice 2D di valori in scala di grigio da forma(8, 8) a forma (64,).\n",
    "\n",
    "Possiamo quindi suddividere i dati in sottoinsiemi di allenamento e test e inserire un classificatore di vettori di supporto sui campioni del train. Il classificatore adattato può successivamente essere utilizzato per prevedere il valore della cifra per i campioni nel sottoinsieme del test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appiattiamo le immagini\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Creiamo il classificatore\n",
    "clf = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Dividiamo i dati per training e test al 50%\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")\n",
    "\n",
    "# alleniamo il modello\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prevediamo i valori\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo quindi a visualizzare i primi 4 campioni di prova e mostriamo il loro valore numerico di previsione nel titolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, prediction in zip(axes, X_test, predicted):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il risultato sembra abbastanza buono ma avendo cifre molto difficili da catalogare anche a occhio umano, ci conviene prima stampare un report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Classification report for classifier {clf}:\\n\"\n",
    "    f\"{metrics.classification_report(y_test, predicted)}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E poi tracciare una matrice di confusione dei valori delle cifre reali e dei valori delle cifre previste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#finora avevamo visto quest'altro metodo per creare grafici che richiedeva più passaggi:\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predicted, labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ora a vedere al gli altri kernel utilizzabili.\n",
    "- Prima cosa creiamo un set di dati di classificazione bidimensionale con 16 campioni e due classi e tracciamo i campioni con i colori corrispondenti ai rispettivi target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [0.4, -0.7],\n",
    "        [-1.5, -1.0],\n",
    "        [-1.4, -0.9],\n",
    "        [-1.3, -1.2],\n",
    "        [-1.1, -0.2],\n",
    "        [-1.2, -0.4],\n",
    "        [-0.5, 1.2],\n",
    "        [-1.5, 2.1],\n",
    "        [1.0, 1.0],\n",
    "        [1.3, 0.8],\n",
    "        [1.2, 0.5],\n",
    "        [0.2, -2.0],\n",
    "        [0.5, -2.4],\n",
    "        [0.2, -2.3],\n",
    "        [0.0, -2.7],\n",
    "        [1.3, 2.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Plotting settings\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "x_min, x_max, y_min, y_max = -3, 3, -3, 3\n",
    "ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "\n",
    "# Plot samples by color and add legend\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], s=150, c=y, label=y, edgecolors=\"k\")\n",
    "ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
    "ax.set_title(\"Samples in two-dimensional feature space\")\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere che i campioni non sono chiaramente separabili da una linea retta.\n",
    "Definiamo  a questo punto una funzione che si adatta a un SVM, consentendo il parametro kernel come input, quindi tracciamo i confini decisionali appresi dal modello utilizzando DecisionBoundaryDisplay.\n",
    "\n",
    "Per semplicità, in questo esempio il parametro C è impostato sul suo valore predefinito ( C=1) su tutti i kernel e la gamma=2. In un'attività di classificazione reale, in cui le prestazioni contano, è altamente consigliabile l'ottimizzazione dei parametri per acquisire diverse strutture all'interno dei dati.\n",
    "\n",
    "Impostiamo response_method=\"predict\" dei colori DecisionBoundaryDisplay in base alle aree della classe prevista. L'utilizzo response_method=\"decision_function\" ci consente anche di tracciare il confine della decisione e i margini su entrambi i lati di esso. Infine i vettori di supporto utilizzati durante l'addestramento (che giacciono sempre ai margini) vengono identificati mediante l'attributo support_vectors_ degli SVM addestrati e anche tracciati.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "def plot_training_data_with_decision_boundary(kernel):\n",
    "    # Train the SVC\n",
    "    clf = svm.SVC(kernel=kernel, gamma=2).fit(X, y)\n",
    "\n",
    "    # Settings for plotting\n",
    "    _, ax = plt.subplots(figsize=(4, 3))\n",
    "    x_min, x_max, y_min, y_max = -3, 3, -3, 3\n",
    "    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    common_params = {\"estimator\": clf, \"X\": X, \"ax\": ax}\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        **common_params,\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        **common_params,\n",
    "        response_method=\"decision_function\",\n",
    "        plot_method=\"contour\",\n",
    "        levels=[-1, 0, 1],\n",
    "        colors=[\"k\", \"k\", \"k\"],\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "    )\n",
    "\n",
    "    # Plot bigger circles around samples that serve as support vectors\n",
    "    ax.scatter(\n",
    "        clf.support_vectors_[:, 0],\n",
    "        clf.support_vectors_[:, 1],\n",
    "        s=250,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"k\",\n",
    "    )\n",
    "    # Plot samples by color and add legend\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=150, edgecolors=\"k\")\n",
    "    ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
    "    ax.set_title(f\" Decision boundaries of {kernel} kernel in SVC\")\n",
    "\n",
    "    _ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rivediamo il kernel lineare nel quale l'addestramento produce uno spazio di caratteristiche non trasformato, dove l'iperpiano e i margini sono linee rette. A causa della mancanza di espressività del kernel lineare, le classi addestrate non catturano perfettamente i dati di addestramento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ora vediamo il kernel polinomiale in cui la nozione di somiglianza è smussata:\n",
    "\n",
    "Qui utilizziamo il valore predefinito per il grado del polinomio nella funzione kernel (degree=3). Quando coef0=0 (impostazione predefinita), i dati vengono solo trasformati, ma non viene aggiunta alcuna dimensione aggiuntiva. Usare un kernel polinomiale equivale a creare PolynomialFeatures e quindi adattare un kernel lineare ai dati trasformati, sebbene questo approccio alternativo sarebbe costoso dal punto di vista computazionale per la maggior parte dei set di dati. \n",
    "Come vediamo il nucleo polinomiale gamma=2 si adatta bene ai dati di addestramento, facendo piegare di conseguenza i margini su entrambi i lati dell'iperpiano.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"poly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poi abbiamo il kernel RBF (Radial Basis Function), noto anche come kernel gaussiano.\n",
    "\n",
    "E' il kernel predefinito per Support Vector Machines in scikit-learn, come sappiamo misura la somiglianza tra due punti dati in dimensioni infinite e quindi si avvicina alla classificazione in base al voto a maggioranza.\n",
    "Nel grafico possiamo vedere come i confini decisionali tendono a contrarsi attorno a punti dati vicini tra loro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid kernel\n",
    "Possiamo vedere che i confini decisionali ottenuti con il nucleo sigmoideo appaiono curvi e irregolari. Il confine decisionale tenta di separare le classi adattando una curva a forma di sigmoide, risultando in un confine complesso che potrebbe non generalizzarsi bene ai dati invisibili. Da questo esempio diventa ovvio che il kernel sigmoideo ha casi d'uso molto specifici, quando si tratta di dati che presentano una forma sigmoidale. In questo esempio, un’attenta messa a punto potrebbe trovare limiti decisionali più generalizzabili. A causa della sua specificità, il kernel sigmoideo è usato meno comunemente nella pratica rispetto ad altri kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 1\n",
    "\n",
    "L'obiettivo di questo esercizio è identificare il kernel migliore dell'algoritmo SVM per classificare il tipo di fiore \"setosa\" e \"virginica\" del dataset iris.\n",
    "Di seguito la base per importare il dataset e le classi specifiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 1, :2]\n",
    "y = y[y != 1]\n",
    "\n",
    "#grafico per vedere il dataset nella totalità\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
    "_ = ax.legend(\n",
    "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempio reale di utilizzo di SVM per Riconoscimento Facciale\n",
    "Come esempio di macchine vettoriali di supporto in azione, diamo un'occhiata al problema del riconoscimento facciale, utilizzeremo il set di dati Labeled Faces in the Wild, che consiste di diverse migliaia di foto raccolte di vari personaggi pubblici che è già incluso in Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "import matplotlib.pyplot as plt\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "#alcune delle immagini presenti\n",
    "print(faces.target_names[0])\n",
    "print(faces.target_names[1])\n",
    "print(faces.target_names[2])\n",
    "\n",
    "#dimensioni dataset\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ora a vedere alcuni di questi volti per capire con cosa stiamo lavorando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 4)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni immagine contiene [62×47] quasi 3.000 pixel. Potremmo procedere semplicemente utilizzando ciascun valore di pixel come caratteristica, ma spesso è più efficace utilizzare una sorta di preprocessore per estrarre caratteristiche più significative; qui utilizzeremo un'analisi delle componenti principali per estrarre 150 componenti fondamentali da inserire nel nostro classificatore di macchine vettoriali di supporto. \n",
    "Possiamo farlo nel modo più semplice impacchettando il preprocessore e il classificatore in un'unica pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=150, whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per testare l'output del nostro classificatore, divideremo i dati in un set in training e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, possiamo utilizzare una convalida incrociata della ricerca sulla griglia per esplorare combinazioni di parametri. Qui regoleremo C (che come sappiamo controlla la durezza del margine) e gamma (che controlla la dimensione del nucleo della funzione base radiale) e determineremo il modello migliore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid, n_jobs=-1)\n",
    "\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori ottimali si trovano verso la metà della nostra griglia, se si trovassero ai bordi, dovremmo espandere la griglia per essere sicuri di aver trovato il vero ottimale.\n",
    "\n",
    "Ora, con questo modello a convalida incrociata, possiamo prevedere le etichette per i dati di test, che il modello non ha ancora visto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diamo un'occhiata ad alcune delle immagini di prova insieme ai loro valori previsti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di questo piccolo campione, il nostro stimatore ottimale ha etichettato erroneamente solo due facce. Possiamo avere un'idea migliore delle prestazioni del nostro stimatore utilizzando il rapporto di classificazione, che elenca le statistiche di recupero etichetta per etichetta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, yfit,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potremmo anche visualizzare la matrice di confusione tra queste classi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo ci aiuta a capire quali etichette potrebbero essere confuse dallo stimatore.\n",
    "\n",
    "Per un'attività di riconoscimento facciale nel mondo reale, in cui le foto non vengono preritagliate in belle griglie, l'unica differenza nello schema di classificazione facciale è la selezione delle funzionalità: dovremmo utilizzare un algoritmo più sofisticato per trovare i volti ed estrarre funzionalità indipendenti dalla pixellazione. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 2\n",
    "Utilizzate il dataset \"Wine\" disponibile nella libreria scikit-learn. L'obiettivo sarà classificare il tipo di vino in base alle sue caratteristiche chimiche.\n",
    "\n",
    "### Passaggi da fare:\n",
    "\n",
    "- Caricamento dei dati:\n",
    "\n",
    "1. Carica il dataset \"Wine\" dalla libreria scikit-learn.\n",
    "2. Esplora il dataset per comprendere le caratteristiche presenti, i loro tipi e la distribuzione delle classi di output.\n",
    "\n",
    "- Preprocessing dei dati:\n",
    "\n",
    "1. Dividi il dataset in features (variabili indipendenti) e target (variabile dipendente).\n",
    "2. Dividi il dataset in training set e test set utilizzando una proporzione del 80-20.\n",
    "\n",
    "- Standardizzazione dei dati:\n",
    "\n",
    "1. Standardizza le features utilizzando lo StandardScaler di scikit-learn.\n",
    "\n",
    "- Creazione del modello SVM:\n",
    "\n",
    "1. Crea un modello SVM utilizzando il kernel lineare.\n",
    "\n",
    "- Addestramento del modello:\n",
    "\n",
    "1. Addestra il modello SVM sul training set.\n",
    "\n",
    "- Valutazione del modello:\n",
    "\n",
    "1. Valuta le prestazioni del modello utilizzando il test set.\n",
    "2. Calcola l'accuratezza del modello.\n",
    "3. Visualizza il report di classificazione che include precision, recall e F1-score per ogni classe.\n",
    "3. Visualizza la matrice di confusione per valutare le prestazioni del modello.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
